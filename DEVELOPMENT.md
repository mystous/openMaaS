# openMaaS ê°œë°œ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” openMaaS í”„ë¡œì íŠ¸ì˜ ë‹¨ê³„ë³„ ê°œë°œ ê³„íšê³¼ ìƒì„¸ ì„¤ê³„ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

---

## ëª©ì°¨

1. [1ë‹¨ê³„: í”„ë¡œì íŠ¸ ê¸°ë°˜ êµ¬ì¶•](#1ë‹¨ê³„-í”„ë¡œì íŠ¸-ê¸°ë°˜-êµ¬ì¶•)
2. [2ë‹¨ê³„: API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ](#2ë‹¨ê³„-api-í‚¤-ê´€ë¦¬-ì‹œìŠ¤í…œ)
3. [3ë‹¨ê³„: ì§ì ‘ í˜¸ì¶œ ì–´ëŒ‘í„°](#3ë‹¨ê³„-ì§ì ‘-í˜¸ì¶œ-ì–´ëŒ‘í„°)
4. [4ë‹¨ê³„: ê¸°ë³¸ ì±„íŒ… UI](#4ë‹¨ê³„-ê¸°ë³¸-ì±„íŒ…-ui)
5. [5ë‹¨ê³„: Pass-through í”„ë¡ì‹œ](#5ë‹¨ê³„-pass-through-í”„ë¡ì‹œ)
6. [6ë‹¨ê³„: ë¹„ìš© ì¶”ì  + ë¹„êµ ëª¨ë“œ](#6ë‹¨ê³„-ë¹„ìš©-ì¶”ì --ë¹„êµ-ëª¨ë“œ)

---

## 1ë‹¨ê³„: í”„ë¡œì íŠ¸ ê¸°ë°˜ êµ¬ì¶•

### ëª©í‘œ
- ëª¨ë…¸ë ˆí¬ êµ¬ì¡° ì„¤ì •
- ê°œë°œ í™˜ê²½ êµ¬ì„±
- ê¸°ë³¸ ì‹¤í–‰ í™˜ê²½ í™•ì¸

### ì˜ˆìƒ ì†Œìš”: 1-2ì¼

### 1-1. ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```
openMaaS/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                      # Next.js í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â””â”€â”€ proxy/                    # FastAPI Pass-through í”„ë¡ì‹œ
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ shared-types/             # ê³µìœ  íƒ€ì… ì •ì˜
â”‚   â””â”€â”€ pricing/                  # ëª¨ë¸ ê°€ê²© ë°ì´í„°
â”œâ”€â”€ docs/                         # ê¸°íš ë¬¸ì„œ (PDF)
â”œâ”€â”€ PROJECT.md
â”œâ”€â”€ DEVELOPMENT.md                # ì´ ë¬¸ì„œ
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ package.json                  # ë£¨íŠ¸ ì›Œí¬ìŠ¤í˜ì´ìŠ¤
â””â”€â”€ pnpm-workspace.yaml
```

### 1-2. Next.js 15 ì´ˆê¸°í™”

```bash
# apps/web ë””ë ‰í† ë¦¬ì—ì„œ
npx create-next-app@latest . --typescript --tailwind --eslint --app --src-dir --import-alias "@/*"
```

**ì„¤ì • ì˜µì…˜**:
- TypeScript: Yes
- ESLint: Yes
- Tailwind CSS: Yes
- `src/` directory: Yes
- App Router: Yes
- Import alias: `@/*`

### 1-3. shadcn/ui ì„¤ì •

```bash
npx shadcn@latest init
```

**ì´ˆê¸° ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜**:
```bash
npx shadcn@latest add button input textarea select dropdown-menu dialog card tabs scroll-area
```

### 1-4. í•„ìˆ˜ ì˜ì¡´ì„± (í”„ë¡ íŠ¸ì—”ë“œ)

```json
{
  "dependencies": {
    "next": "^15.0.0",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "zustand": "^5.0.0",
    "openai": "^4.70.0",
    "@google/generative-ai": "^0.21.0",
    "react-markdown": "^9.0.0",
    "remark-gfm": "^4.0.0",
    "rehype-highlight": "^7.0.0",
    "tiktoken": "^1.0.0",
    "lucide-react": "^0.460.0"
  },
  "devDependencies": {
    "@types/node": "^22.0.0",
    "@types/react": "^19.0.0",
    "typescript": "^5.6.0",
    "tailwindcss": "^3.4.0",
    "eslint": "^9.0.0",
    "prettier": "^3.4.0"
  }
}
```

### 1-5. FastAPI í”„ë¡œì íŠ¸ ì´ˆê¸°í™”

```bash
# apps/proxy ë””ë ‰í† ë¦¬ì—ì„œ
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

**requirements.txt**:
```
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
httpx>=0.28.0
pydantic>=2.10.0
python-dotenv>=1.0.0
```

**ê¸°ë³¸ êµ¬ì¡°**:
```
apps/proxy/
â”œâ”€â”€ main.py
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ completions.py
â”œâ”€â”€ adapters/
â”‚   â””â”€â”€ base.py
â”œâ”€â”€ middleware/
â”‚   â””â”€â”€ security.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

### 1-6. Docker Compose ê¸°ë³¸ ì„¤ì •

```yaml
# docker-compose.yml
version: '3.9'

services:
  web:
    build:
      context: ./apps/web
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_PROXY_URL=http://proxy:8000
    depends_on:
      - proxy

  proxy:
    build:
      context: ./apps/proxy
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - LOG_LEVEL=info
      # API í‚¤ ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ ì—†ìŒ (Zero-Knowledge)
```

### 1-7. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `pnpm install` ì„±ê³µ
- [ ] `pnpm dev` (web) ì‹¤í–‰ â†’ localhost:3000 ì ‘ì† ê°€ëŠ¥
- [ ] `uvicorn main:app --reload` (proxy) ì‹¤í–‰ â†’ localhost:8000/docs ì ‘ì† ê°€ëŠ¥
- [ ] `docker-compose up` ì„±ê³µ

---

## 2ë‹¨ê³„: API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ

### ëª©í‘œ
- í´ë¼ì´ì–¸íŠ¸ ì¸¡ API í‚¤ ì €ì¥/ê´€ë¦¬ êµ¬í˜„
- ì œê³µìë³„ í‚¤ ì…ë ¥ UI

### ì˜ˆìƒ ì†Œìš”: 2-3ì¼

### 2-1. ì œê³µì íƒ€ì… ì •ì˜

```typescript
// packages/shared-types/src/providers.ts

export type ProviderId =
  | 'openai'
  | 'anthropic'
  | 'gemini'
  | 'bedrock'
  | 'azure'
  | 'vertex'
  | 'mistral'
  | 'cohere'
  | 'ollama';

export interface ProviderConfig {
  id: ProviderId;
  name: string;
  description: string;
  supportsCORS: boolean;           // trueë©´ ì§ì ‘ í˜¸ì¶œ, falseë©´ í”„ë¡ì‹œ
  keyPlaceholder: string;          // "sk-..." ë“±
  keyPattern?: RegExp;             // í‚¤ í˜•ì‹ ê²€ì¦
  docsUrl: string;                 // API í‚¤ ë°œê¸‰ ë¬¸ì„œ ë§í¬
  models: ModelConfig[];
}

export interface ModelConfig {
  id: string;                      // "gpt-4o", "claude-3-5-sonnet" ë“±
  name: string;
  contextWindow: number;
  pricing: {
    input: number;                 // $ per 1M tokens
    output: number;
    cached?: number;               // ìºì‹œëœ ì…ë ¥ í† í°
  };
  capabilities: {
    vision: boolean;
    tools: boolean;
    streaming: boolean;
  };
}

// ì œê³µì ì„¤ì • ëª©ë¡
export const PROVIDERS: Record<ProviderId, ProviderConfig> = {
  openai: {
    id: 'openai',
    name: 'OpenAI',
    description: 'GPT-4o, GPT-4 Turbo, GPT-3.5',
    supportsCORS: true,
    keyPlaceholder: 'sk-...',
    keyPattern: /^sk-[a-zA-Z0-9]{32,}$/,
    docsUrl: 'https://platform.openai.com/api-keys',
    models: [
      {
        id: 'gpt-4o',
        name: 'GPT-4o',
        contextWindow: 128000,
        pricing: { input: 2.5, output: 10 },
        capabilities: { vision: true, tools: true, streaming: true }
      },
      // ... ë” ë§ì€ ëª¨ë¸
    ]
  },
  anthropic: {
    id: 'anthropic',
    name: 'Anthropic',
    description: 'Claude 3.5 Sonnet, Claude 3 Opus',
    supportsCORS: false,  // í”„ë¡ì‹œ í•„ìš”
    keyPlaceholder: 'sk-ant-...',
    keyPattern: /^sk-ant-[a-zA-Z0-9-]{32,}$/,
    docsUrl: 'https://console.anthropic.com/settings/keys',
    models: [
      {
        id: 'claude-3-5-sonnet-20241022',
        name: 'Claude 3.5 Sonnet',
        contextWindow: 200000,
        pricing: { input: 3, output: 15 },
        capabilities: { vision: true, tools: true, streaming: true }
      },
      // ...
    ]
  },
  gemini: {
    id: 'gemini',
    name: 'Google Gemini',
    description: 'Gemini 1.5 Pro, Gemini 2.0 Flash',
    supportsCORS: true,
    keyPlaceholder: 'AIza...',
    keyPattern: /^AIza[a-zA-Z0-9_-]{35}$/,
    docsUrl: 'https://aistudio.google.com/app/apikey',
    models: [/* ... */]
  },
  ollama: {
    id: 'ollama',
    name: 'Ollama (ë¡œì»¬)',
    description: 'Llama 3, Mistral, Qwen ë“± ë¡œì»¬ ëª¨ë¸',
    supportsCORS: true,
    keyPlaceholder: '(API í‚¤ ë¶ˆí•„ìš”)',
    docsUrl: 'https://ollama.ai/',
    models: [/* ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜´ */]
  },
  // ... anthropic, bedrock, azure, vertex, mistral, cohere
};
```

### 2-2. KeyManager í´ë˜ìŠ¤

```typescript
// apps/web/src/lib/llm/key-manager.ts

export type StorageType = 'local' | 'session' | 'memory';

interface StoredKey {
  key: string;
  addedAt: number;
  lastUsedAt?: number;
}

class KeyManager {
  private memoryStore: Map<ProviderId, StoredKey> = new Map();
  private readonly STORAGE_PREFIX = 'openmaas_key_';

  /**
   * API í‚¤ ì €ì¥
   * @param provider ì œê³µì ID
   * @param key API í‚¤ (í‰ë¬¸)
   * @param storage ì €ì¥ ìœ„ì¹˜
   */
  setKey(provider: ProviderId, key: string, storage: StorageType = 'local'): void {
    const stored: StoredKey = {
      key,
      addedAt: Date.now(),
    };

    switch (storage) {
      case 'local':
        localStorage.setItem(
          this.STORAGE_PREFIX + provider,
          JSON.stringify(stored)
        );
        break;
      case 'session':
        sessionStorage.setItem(
          this.STORAGE_PREFIX + provider,
          JSON.stringify(stored)
        );
        break;
      case 'memory':
        this.memoryStore.set(provider, stored);
        break;
    }
  }

  /**
   * API í‚¤ ì¡°íšŒ
   */
  getKey(provider: ProviderId): string | null {
    // 1. ë©”ëª¨ë¦¬ ë¨¼ì € í™•ì¸
    const memoryKey = this.memoryStore.get(provider);
    if (memoryKey) return memoryKey.key;

    // 2. sessionStorage í™•ì¸
    const sessionData = sessionStorage.getItem(this.STORAGE_PREFIX + provider);
    if (sessionData) {
      const parsed: StoredKey = JSON.parse(sessionData);
      return parsed.key;
    }

    // 3. localStorage í™•ì¸
    const localData = localStorage.getItem(this.STORAGE_PREFIX + provider);
    if (localData) {
      const parsed: StoredKey = JSON.parse(localData);
      return parsed.key;
    }

    return null;
  }

  /**
   * í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
   */
  hasKey(provider: ProviderId): boolean {
    return this.getKey(provider) !== null;
  }

  /**
   * í‚¤ ì‚­ì œ
   */
  deleteKey(provider: ProviderId): void {
    this.memoryStore.delete(provider);
    sessionStorage.removeItem(this.STORAGE_PREFIX + provider);
    localStorage.removeItem(this.STORAGE_PREFIX + provider);
  }

  /**
   * ì„¤ì •ëœ ëª¨ë“  ì œê³µì ëª©ë¡
   */
  getConfiguredProviders(): ProviderId[] {
    return Object.keys(PROVIDERS).filter(
      (id) => this.hasKey(id as ProviderId)
    ) as ProviderId[];
  }

  /**
   * ëª¨ë“  í‚¤ ì‚­ì œ
   */
  clearAll(): void {
    this.memoryStore.clear();
    Object.keys(PROVIDERS).forEach((id) => {
      sessionStorage.removeItem(this.STORAGE_PREFIX + id);
      localStorage.removeItem(this.STORAGE_PREFIX + id);
    });
  }
}

export const keyManager = new KeyManager();
```

### 2-3. Zustand ìŠ¤í† ì–´

```typescript
// apps/web/src/lib/store/key-store.ts

import { create } from 'zustand';
import { keyManager, StorageType } from '../llm/key-manager';
import { ProviderId, PROVIDERS } from '@openmaas/shared-types';

interface KeyState {
  // ìƒíƒœ
  configuredProviders: ProviderId[];

  // ì•¡ì…˜
  addKey: (provider: ProviderId, key: string, storage: StorageType) => void;
  removeKey: (provider: ProviderId) => void;
  hasKey: (provider: ProviderId) => boolean;
  getKey: (provider: ProviderId) => string | null;
  refreshConfigured: () => void;
}

export const useKeyStore = create<KeyState>((set, get) => ({
  configuredProviders: [],

  addKey: (provider, key, storage) => {
    keyManager.setKey(provider, key, storage);
    set({ configuredProviders: keyManager.getConfiguredProviders() });
  },

  removeKey: (provider) => {
    keyManager.deleteKey(provider);
    set({ configuredProviders: keyManager.getConfiguredProviders() });
  },

  hasKey: (provider) => keyManager.hasKey(provider),

  getKey: (provider) => keyManager.getKey(provider),

  refreshConfigured: () => {
    set({ configuredProviders: keyManager.getConfiguredProviders() });
  },
}));
```

### 2-4. ì„¤ì • í˜ì´ì§€ UI

```typescript
// apps/web/src/app/settings/page.tsx

'use client';

import { useState } from 'react';
import { useKeyStore } from '@/lib/store/key-store';
import { PROVIDERS, ProviderId } from '@openmaas/shared-types';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardHeader, CardTitle, CardContent } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Eye, EyeOff, Trash2, ExternalLink, Check } from 'lucide-react';

export default function SettingsPage() {
  const { configuredProviders, addKey, removeKey, hasKey } = useKeyStore();

  return (
    <div className="container max-w-4xl py-8">
      <h1 className="text-3xl font-bold mb-8">API í‚¤ ì„¤ì •</h1>

      <div className="space-y-4">
        {Object.values(PROVIDERS).map((provider) => (
          <ProviderKeyCard
            key={provider.id}
            provider={provider}
            isConfigured={hasKey(provider.id)}
            onSave={(key) => addKey(provider.id, key, 'local')}
            onRemove={() => removeKey(provider.id)}
          />
        ))}
      </div>

      <div className="mt-8 p-4 bg-muted rounded-lg">
        <h3 className="font-semibold mb-2">ğŸ”’ ë³´ì•ˆ ì•ˆë‚´</h3>
        <ul className="text-sm text-muted-foreground space-y-1">
          <li>â€¢ API í‚¤ëŠ” ë¸Œë¼ìš°ì €ì˜ localStorageì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤.</li>
          <li>â€¢ ì„œë²„ë¡œ ì „ì†¡ë˜ê±°ë‚˜ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</li>
          <li>â€¢ ê³µìš© ì»´í“¨í„°ì—ì„œëŠ” ì„¸ì…˜ ì €ì¥ì†Œ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</li>
        </ul>
      </div>
    </div>
  );
}

function ProviderKeyCard({ provider, isConfigured, onSave, onRemove }) {
  const [key, setKey] = useState('');
  const [showKey, setShowKey] = useState(false);
  const [testing, setTesting] = useState(false);

  const handleSave = () => {
    if (key.trim()) {
      onSave(key.trim());
      setKey('');
    }
  };

  const testConnection = async () => {
    setTesting(true);
    // TODO: ì‹¤ì œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
    await new Promise((r) => setTimeout(r, 1000));
    setTesting(false);
  };

  return (
    <Card>
      <CardHeader className="flex flex-row items-center justify-between">
        <div>
          <CardTitle className="flex items-center gap-2">
            {provider.name}
            {provider.supportsCORS ? (
              <Badge variant="outline" className="text-xs">ì§ì ‘ í˜¸ì¶œ</Badge>
            ) : (
              <Badge variant="secondary" className="text-xs">í”„ë¡ì‹œ</Badge>
            )}
          </CardTitle>
          <p className="text-sm text-muted-foreground">{provider.description}</p>
        </div>
        {isConfigured && (
          <Badge variant="default" className="bg-green-600">
            <Check className="w-3 h-3 mr-1" /> ì„¤ì •ë¨
          </Badge>
        )}
      </CardHeader>
      <CardContent>
        {isConfigured ? (
          <div className="flex items-center gap-2">
            <Input value="â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢" disabled className="font-mono" />
            <Button variant="outline" onClick={testConnection} disabled={testing}>
              {testing ? 'í…ŒìŠ¤íŠ¸ ì¤‘...' : 'í…ŒìŠ¤íŠ¸'}
            </Button>
            <Button variant="destructive" size="icon" onClick={onRemove}>
              <Trash2 className="w-4 h-4" />
            </Button>
          </div>
        ) : (
          <div className="flex items-center gap-2">
            <Input
              type={showKey ? 'text' : 'password'}
              value={key}
              onChange={(e) => setKey(e.target.value)}
              placeholder={provider.keyPlaceholder}
              className="font-mono"
            />
            <Button
              variant="ghost"
              size="icon"
              onClick={() => setShowKey(!showKey)}
            >
              {showKey ? <EyeOff className="w-4 h-4" /> : <Eye className="w-4 h-4" />}
            </Button>
            <Button onClick={handleSave} disabled={!key.trim()}>
              ì €ì¥
            </Button>
            <Button variant="link" size="sm" asChild>
              <a href={provider.docsUrl} target="_blank" rel="noopener">
                <ExternalLink className="w-3 h-3 mr-1" />
                í‚¤ ë°œê¸‰
              </a>
            </Button>
          </div>
        )}
      </CardContent>
    </Card>
  );
}
```

### 2-5. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `/settings` í˜ì´ì§€ ì ‘ì† ê°€ëŠ¥
- [ ] ê° ì œê³µìë³„ API í‚¤ ì…ë ¥ ê°€ëŠ¥
- [ ] ì…ë ¥í•œ í‚¤ê°€ localStorageì— ì €ì¥ë¨
- [ ] í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ì—ë„ í‚¤ ìœ ì§€
- [ ] í‚¤ ì‚­ì œ ê¸°ëŠ¥ ë™ì‘
- [ ] í‚¤ ì…ë ¥ ì‹œ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬

---

## 3ë‹¨ê³„: ì§ì ‘ í˜¸ì¶œ ì–´ëŒ‘í„°

### ëª©í‘œ
- CORS ì§€ì› ì œê³µì ë¸Œë¼ìš°ì € ì§ì ‘ í˜¸ì¶œ êµ¬í˜„
- OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ì •ê·œí™”

### ì˜ˆìƒ ì†Œìš”: 3-4ì¼

### 3-1. BaseAdapter ì¸í„°í˜ì´ìŠ¤

```typescript
// apps/web/src/lib/llm/adapters/base.ts

import { ProviderId } from '@openmaas/shared-types';

// OpenAI í˜¸í™˜ ë©”ì‹œì§€ í˜•ì‹
export interface Message {
  role: 'system' | 'user' | 'assistant' | 'tool';
  content: string | ContentPart[];
  name?: string;
  tool_calls?: ToolCall[];
  tool_call_id?: string;
}

export interface ContentPart {
  type: 'text' | 'image_url';
  text?: string;
  image_url?: { url: string };
}

export interface ToolCall {
  id: string;
  type: 'function';
  function: { name: string; arguments: string };
}

// ìš”ì²­ íŒŒë¼ë¯¸í„°
export interface ChatRequest {
  model: string;
  messages: Message[];
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  stream?: boolean;
  tools?: Tool[];
  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
}

// ì‘ë‹µ í˜•ì‹ (OpenAI í˜¸í™˜)
export interface ChatResponse {
  id: string;
  object: 'chat.completion';
  created: number;
  model: string;
  choices: Choice[];
  usage: Usage;
}

export interface Choice {
  index: number;
  message: Message;
  finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter';
}

export interface Usage {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
}

// ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
export interface ChatChunk {
  id: string;
  object: 'chat.completion.chunk';
  created: number;
  model: string;
  choices: ChunkChoice[];
}

export interface ChunkChoice {
  index: number;
  delta: Partial<Message>;
  finish_reason: string | null;
}

// ì–´ëŒ‘í„° ì¸í„°í˜ì´ìŠ¤
export interface LLMAdapter {
  readonly providerId: ProviderId;
  readonly supportsCORS: boolean;

  // ì¼ë°˜ í˜¸ì¶œ
  chat(request: ChatRequest, apiKey: string): Promise<ChatResponse>;

  // ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
  chatStream(request: ChatRequest, apiKey: string): AsyncIterable<ChatChunk>;

  // ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (Ollama ë“± ë™ì  ëª¨ë¸ìš©)
  listModels?(apiKey: string): Promise<string[]>;
}
```

### 3-2. OpenAI ì–´ëŒ‘í„°

```typescript
// apps/web/src/lib/llm/adapters/openai.ts

import OpenAI from 'openai';
import { LLMAdapter, ChatRequest, ChatResponse, ChatChunk } from './base';

export class OpenAIAdapter implements LLMAdapter {
  readonly providerId = 'openai' as const;
  readonly supportsCORS = true;

  private createClient(apiKey: string): OpenAI {
    return new OpenAI({
      apiKey,
      dangerouslyAllowBrowser: true,  // ë¸Œë¼ìš°ì € ì§ì ‘ í˜¸ì¶œ í—ˆìš©
    });
  }

  async chat(request: ChatRequest, apiKey: string): Promise<ChatResponse> {
    const client = this.createClient(apiKey);

    const response = await client.chat.completions.create({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature,
      max_tokens: request.max_tokens,
      top_p: request.top_p,
      tools: request.tools,
      tool_choice: request.tool_choice,
      stream: false,
    });

    return response as ChatResponse;
  }

  async *chatStream(request: ChatRequest, apiKey: string): AsyncIterable<ChatChunk> {
    const client = this.createClient(apiKey);

    const stream = await client.chat.completions.create({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature,
      max_tokens: request.max_tokens,
      top_p: request.top_p,
      stream: true,
    });

    for await (const chunk of stream) {
      yield chunk as ChatChunk;
    }
  }

  async listModels(apiKey: string): Promise<string[]> {
    const client = this.createClient(apiKey);
    const models = await client.models.list();

    return models.data
      .filter((m) => m.id.startsWith('gpt'))
      .map((m) => m.id)
      .sort();
  }
}
```

### 3-3. Gemini ì–´ëŒ‘í„°

```typescript
// apps/web/src/lib/llm/adapters/gemini.ts

import { GoogleGenerativeAI, GenerativeModel } from '@google/generative-ai';
import { LLMAdapter, ChatRequest, ChatResponse, ChatChunk, Message } from './base';

export class GeminiAdapter implements LLMAdapter {
  readonly providerId = 'gemini' as const;
  readonly supportsCORS = true;

  private createClient(apiKey: string): GoogleGenerativeAI {
    return new GoogleGenerativeAI(apiKey);
  }

  // OpenAI ë©”ì‹œì§€ â†’ Gemini Content ë³€í™˜
  private convertMessages(messages: Message[]): { role: string; parts: { text: string }[] }[] {
    const geminiMessages: { role: string; parts: { text: string }[] }[] = [];
    let systemInstruction = '';

    for (const msg of messages) {
      if (msg.role === 'system') {
        // GeminiëŠ” systemì„ ë³„ë„ë¡œ ì²˜ë¦¬
        systemInstruction += (typeof msg.content === 'string' ? msg.content : '') + '\n';
      } else {
        geminiMessages.push({
          role: msg.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: typeof msg.content === 'string' ? msg.content : '' }],
        });
      }
    }

    // System instructionì„ ì²« user ë©”ì‹œì§€ì— prepend
    if (systemInstruction && geminiMessages.length > 0) {
      const firstUserIdx = geminiMessages.findIndex((m) => m.role === 'user');
      if (firstUserIdx >= 0) {
        geminiMessages[firstUserIdx].parts[0].text =
          `[System Instructions]\n${systemInstruction}\n\n${geminiMessages[firstUserIdx].parts[0].text}`;
      }
    }

    return geminiMessages;
  }

  async chat(request: ChatRequest, apiKey: string): Promise<ChatResponse> {
    const client = this.createClient(apiKey);
    const model = client.getGenerativeModel({ model: request.model });

    const geminiMessages = this.convertMessages(request.messages);

    // ë§ˆì§€ë§‰ user ë©”ì‹œì§€ë¥¼ promptë¡œ, ë‚˜ë¨¸ì§€ë¥¼ historyë¡œ
    const history = geminiMessages.slice(0, -1);
    const lastMessage = geminiMessages[geminiMessages.length - 1];

    const chat = model.startChat({
      history,
      generationConfig: {
        temperature: request.temperature,
        maxOutputTokens: request.max_tokens,
        topP: request.top_p,
      },
    });

    const result = await chat.sendMessage(lastMessage.parts[0].text);
    const response = result.response;

    // OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return {
      id: `gemini-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: request.model,
      choices: [
        {
          index: 0,
          message: {
            role: 'assistant',
            content: response.text(),
          },
          finish_reason: 'stop',
        },
      ],
      usage: {
        prompt_tokens: response.usageMetadata?.promptTokenCount ?? 0,
        completion_tokens: response.usageMetadata?.candidatesTokenCount ?? 0,
        total_tokens: response.usageMetadata?.totalTokenCount ?? 0,
      },
    };
  }

  async *chatStream(request: ChatRequest, apiKey: string): AsyncIterable<ChatChunk> {
    const client = this.createClient(apiKey);
    const model = client.getGenerativeModel({ model: request.model });

    const geminiMessages = this.convertMessages(request.messages);
    const history = geminiMessages.slice(0, -1);
    const lastMessage = geminiMessages[geminiMessages.length - 1];

    const chat = model.startChat({
      history,
      generationConfig: {
        temperature: request.temperature,
        maxOutputTokens: request.max_tokens,
      },
    });

    const streamResult = await chat.sendMessageStream(lastMessage.parts[0].text);
    const id = `gemini-${Date.now()}`;
    const created = Math.floor(Date.now() / 1000);

    for await (const chunk of streamResult.stream) {
      const text = chunk.text();
      if (text) {
        yield {
          id,
          object: 'chat.completion.chunk',
          created,
          model: request.model,
          choices: [
            {
              index: 0,
              delta: { content: text },
              finish_reason: null,
            },
          ],
        };
      }
    }

    // ë§ˆì§€ë§‰ ì²­í¬
    yield {
      id,
      object: 'chat.completion.chunk',
      created,
      model: request.model,
      choices: [
        {
          index: 0,
          delta: {},
          finish_reason: 'stop',
        },
      ],
    };
  }
}
```

### 3-4. Ollama ì–´ëŒ‘í„°

```typescript
// apps/web/src/lib/llm/adapters/ollama.ts

import { LLMAdapter, ChatRequest, ChatResponse, ChatChunk } from './base';

export class OllamaAdapter implements LLMAdapter {
  readonly providerId = 'ollama' as const;
  readonly supportsCORS = true;

  private baseUrl: string;

  constructor(baseUrl: string = 'http://localhost:11434') {
    this.baseUrl = baseUrl;
  }

  async chat(request: ChatRequest, _apiKey: string): Promise<ChatResponse> {
    const response = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: request.model,
        messages: request.messages,
        stream: false,
        options: {
          temperature: request.temperature,
          num_predict: request.max_tokens,
          top_p: request.top_p,
        },
      }),
    });

    const data = await response.json();

    return {
      id: `ollama-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: request.model,
      choices: [
        {
          index: 0,
          message: {
            role: 'assistant',
            content: data.message.content,
          },
          finish_reason: 'stop',
        },
      ],
      usage: {
        prompt_tokens: data.prompt_eval_count ?? 0,
        completion_tokens: data.eval_count ?? 0,
        total_tokens: (data.prompt_eval_count ?? 0) + (data.eval_count ?? 0),
      },
    };
  }

  async *chatStream(request: ChatRequest, _apiKey: string): AsyncIterable<ChatChunk> {
    const response = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: request.model,
        messages: request.messages,
        stream: true,
        options: {
          temperature: request.temperature,
          num_predict: request.max_tokens,
        },
      }),
    });

    const reader = response.body!.getReader();
    const decoder = new TextDecoder();
    const id = `ollama-${Date.now()}`;
    const created = Math.floor(Date.now() / 1000);

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const text = decoder.decode(value);
      const lines = text.split('\n').filter(Boolean);

      for (const line of lines) {
        const data = JSON.parse(line);

        yield {
          id,
          object: 'chat.completion.chunk',
          created,
          model: request.model,
          choices: [
            {
              index: 0,
              delta: { content: data.message?.content ?? '' },
              finish_reason: data.done ? 'stop' : null,
            },
          ],
        };
      }
    }
  }

  async listModels(): Promise<string[]> {
    const response = await fetch(`${this.baseUrl}/api/tags`);
    const data = await response.json();
    return data.models.map((m: { name: string }) => m.name);
  }
}
```

### 3-5. Provider Router

```typescript
// apps/web/src/lib/llm/provider-router.ts

import { ProviderId, PROVIDERS } from '@openmaas/shared-types';
import { LLMAdapter, ChatRequest, ChatResponse, ChatChunk } from './adapters/base';
import { OpenAIAdapter } from './adapters/openai';
import { GeminiAdapter } from './adapters/gemini';
import { OllamaAdapter } from './adapters/ollama';
import { ProxyAdapter } from './adapters/proxy';
import { keyManager } from './key-manager';

class ProviderRouter {
  private directAdapters: Map<ProviderId, LLMAdapter> = new Map();
  private proxyAdapter: ProxyAdapter;

  constructor(proxyUrl: string = '/api/proxy') {
    // ì§ì ‘ í˜¸ì¶œ ì–´ëŒ‘í„° ë“±ë¡
    this.directAdapters.set('openai', new OpenAIAdapter());
    this.directAdapters.set('gemini', new GeminiAdapter());
    this.directAdapters.set('ollama', new OllamaAdapter());

    // í”„ë¡ì‹œ ì–´ëŒ‘í„° (CORS ë¯¸ì§€ì› ì œê³µììš©)
    this.proxyAdapter = new ProxyAdapter(proxyUrl);
  }

  private getAdapter(provider: ProviderId): LLMAdapter {
    const config = PROVIDERS[provider];

    if (config.supportsCORS) {
      const adapter = this.directAdapters.get(provider);
      if (adapter) return adapter;
    }

    // CORS ë¯¸ì§€ì› ë˜ëŠ” ì§ì ‘ ì–´ëŒ‘í„° ì—†ìŒ â†’ í”„ë¡ì‹œ
    return this.proxyAdapter;
  }

  async chat(provider: ProviderId, request: ChatRequest): Promise<ChatResponse> {
    const apiKey = keyManager.getKey(provider);
    if (!apiKey && provider !== 'ollama') {
      throw new Error(`API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: ${provider}`);
    }

    const adapter = this.getAdapter(provider);

    // í”„ë¡ì‹œ ì–´ëŒ‘í„°ì˜ ê²½ìš° provider ì •ë³´ë„ ì „ë‹¬
    if (adapter instanceof ProxyAdapter) {
      return adapter.chat(request, apiKey ?? '', provider);
    }

    return adapter.chat(request, apiKey ?? '');
  }

  async *chatStream(
    provider: ProviderId,
    request: ChatRequest
  ): AsyncIterable<ChatChunk> {
    const apiKey = keyManager.getKey(provider);
    if (!apiKey && provider !== 'ollama') {
      throw new Error(`API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: ${provider}`);
    }

    const adapter = this.getAdapter(provider);

    if (adapter instanceof ProxyAdapter) {
      yield* adapter.chatStream(request, apiKey ?? '', provider);
    } else {
      yield* adapter.chatStream(request, apiKey ?? '');
    }
  }
}

export const providerRouter = new ProviderRouter(
  process.env.NEXT_PUBLIC_PROXY_URL || '/api/proxy'
);
```

### 3-6. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] OpenAI í‚¤ ì…ë ¥ í›„ GPT-4o í˜¸ì¶œ ì„±ê³µ
- [ ] ì‘ë‹µì´ OpenAI í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”ë¨
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤ì‹œê°„ ì¶œë ¥
- [ ] Gemini ì–´ëŒ‘í„° ë™ì‘ í™•ì¸
- [ ] Ollama ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ ì„±ê³µ
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€

---

## 4ë‹¨ê³„: ê¸°ë³¸ ì±„íŒ… UI

### ëª©í‘œ
- ì±„íŒ… í”Œë ˆì´ê·¸ë¼ìš´ë“œ MVP êµ¬í˜„
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ

### ì˜ˆìƒ ì†Œìš”: 4-5ì¼

### 4-1. ë ˆì´ì•„ì›ƒ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ ì‚¬ì´ë“œë°” â”‚           ë©”ì¸ ì±„íŒ… ì˜ì—­              â”‚ ì„¤ì • íŒ¨ë„   â”‚  â”‚
â”‚ â”‚         â”‚                                       â”‚             â”‚  â”‚
â”‚ â”‚ â€¢ ìƒˆ ì±„íŒ…â”‚ [ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´]                  â”‚ Temperature â”‚  â”‚
â”‚ â”‚ â€¢ ê¸°ë¡1  â”‚                                       â”‚ [0.7    ]   â”‚  â”‚
â”‚ â”‚ â€¢ ê¸°ë¡2  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚  â”‚
â”‚ â”‚ â€¢ ê¸°ë¡3  â”‚ â”‚ User: ì•ˆë…•í•˜ì„¸ìš”                  â”‚ â”‚ Max Tokens  â”‚  â”‚
â”‚ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ [1024   ]   â”‚  â”‚
â”‚ â”‚         â”‚                                       â”‚             â”‚  â”‚
â”‚ â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Top P       â”‚  â”‚
â”‚ â”‚         â”‚ â”‚ Assistant: ì•ˆë…•í•˜ì„¸ìš”!             â”‚ â”‚ [1.0    ]   â”‚  â”‚
â”‚ â”‚         â”‚ â”‚ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?               â”‚ â”‚             â”‚  â”‚
â”‚ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ [ì‹œìŠ¤í…œ     â”‚  â”‚
â”‚ â”‚         â”‚                                       â”‚  í”„ë¡¬í”„íŠ¸]  â”‚  â”‚
â”‚ â”‚         â”‚                                       â”‚             â”‚  â”‚
â”‚ â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚  â”‚
â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚ ë©”ì‹œì§€ ì…ë ¥...              [ì „ì†¡]â”‚ â”‚             â”‚  â”‚
â”‚ â”‚ ì„¤ì •    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                          í† í°: 523 | ë¹„ìš©: $0.02   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4-2. ì±„íŒ… ìƒíƒœ ê´€ë¦¬

```typescript
// apps/web/src/lib/store/chat-store.ts

import { create } from 'zustand';
import { Message, ChatRequest, ChatChunk } from '../llm/adapters/base';
import { providerRouter } from '../llm/provider-router';
import { ProviderId } from '@openmaas/shared-types';

interface ChatMessage extends Message {
  id: string;
  timestamp: number;
  usage?: { prompt_tokens: number; completion_tokens: number };
  latency?: number;  // TTFT in ms
}

interface ChatSession {
  id: string;
  title: string;
  messages: ChatMessage[];
  provider: ProviderId;
  model: string;
  createdAt: number;
  updatedAt: number;
}

interface ChatState {
  // í˜„ì¬ ì„¸ì…˜
  currentSession: ChatSession | null;
  sessions: ChatSession[];

  // ì„¤ì •
  provider: ProviderId;
  model: string;
  temperature: number;
  maxTokens: number;
  topP: number;
  systemPrompt: string;

  // UI ìƒíƒœ
  isStreaming: boolean;
  streamingContent: string;

  // ì•¡ì…˜
  setProvider: (provider: ProviderId) => void;
  setModel: (model: string) => void;
  setTemperature: (temp: number) => void;
  setMaxTokens: (tokens: number) => void;
  setTopP: (topP: number) => void;
  setSystemPrompt: (prompt: string) => void;

  sendMessage: (content: string) => Promise<void>;
  newSession: () => void;
  loadSession: (id: string) => void;
  deleteSession: (id: string) => void;
}

export const useChatStore = create<ChatState>((set, get) => ({
  currentSession: null,
  sessions: [],

  provider: 'openai',
  model: 'gpt-4o',
  temperature: 0.7,
  maxTokens: 4096,
  topP: 1.0,
  systemPrompt: '',

  isStreaming: false,
  streamingContent: '',

  setProvider: (provider) => set({ provider }),
  setModel: (model) => set({ model }),
  setTemperature: (temperature) => set({ temperature }),
  setMaxTokens: (maxTokens) => set({ maxTokens }),
  setTopP: (topP) => set({ topP }),
  setSystemPrompt: (systemPrompt) => set({ systemPrompt }),

  sendMessage: async (content) => {
    const state = get();

    // ì„¸ì…˜ ì—†ìœ¼ë©´ ìƒì„±
    let session = state.currentSession;
    if (!session) {
      session = {
        id: crypto.randomUUID(),
        title: content.slice(0, 30) + '...',
        messages: [],
        provider: state.provider,
        model: state.model,
        createdAt: Date.now(),
        updatedAt: Date.now(),
      };
      set({ currentSession: session });
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    const userMessage: ChatMessage = {
      id: crypto.randomUUID(),
      role: 'user',
      content,
      timestamp: Date.now(),
    };
    session.messages.push(userMessage);
    set({ currentSession: { ...session } });

    // ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
    set({ isStreaming: true, streamingContent: '' });

    const messages: Message[] = [];

    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    if (state.systemPrompt) {
      messages.push({ role: 'system', content: state.systemPrompt });
    }

    // ê¸°ì¡´ ë©”ì‹œì§€ ì¶”ê°€
    messages.push(...session.messages.map((m) => ({
      role: m.role,
      content: m.content,
    })));

    const request: ChatRequest = {
      model: state.model,
      messages,
      temperature: state.temperature,
      max_tokens: state.maxTokens,
      top_p: state.topP,
      stream: true,
    };

    let fullContent = '';
    let ttft: number | undefined;
    const startTime = Date.now();

    try {
      for await (const chunk of providerRouter.chatStream(state.provider, request)) {
        if (!ttft) {
          ttft = Date.now() - startTime;
        }

        const delta = chunk.choices[0]?.delta?.content ?? '';
        fullContent += delta;
        set({ streamingContent: fullContent });
      }

      // ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
      const assistantMessage: ChatMessage = {
        id: crypto.randomUUID(),
        role: 'assistant',
        content: fullContent,
        timestamp: Date.now(),
        latency: ttft,
      };
      session.messages.push(assistantMessage);
      session.updatedAt = Date.now();

      set({
        currentSession: { ...session },
        isStreaming: false,
        streamingContent: '',
      });
    } catch (error) {
      set({ isStreaming: false, streamingContent: '' });
      throw error;
    }
  },

  newSession: () => {
    const state = get();
    if (state.currentSession) {
      set({
        sessions: [...state.sessions, state.currentSession],
        currentSession: null,
      });
    }
  },

  loadSession: (id) => {
    const session = get().sessions.find((s) => s.id === id);
    if (session) {
      set({ currentSession: session });
    }
  },

  deleteSession: (id) => {
    set((state) => ({
      sessions: state.sessions.filter((s) => s.id !== id),
      currentSession: state.currentSession?.id === id ? null : state.currentSession,
    }));
  },
}));
```

### 4-3. ì£¼ìš” ì»´í¬ë„ŒíŠ¸

```typescript
// apps/web/src/components/chat/chat-page.tsx

'use client';

import { ChatSidebar } from './chat-sidebar';
import { ChatMain } from './chat-main';
import { ChatSettings } from './chat-settings';

export function ChatPage() {
  return (
    <div className="flex h-screen">
      <ChatSidebar className="w-64 border-r" />
      <ChatMain className="flex-1" />
      <ChatSettings className="w-80 border-l" />
    </div>
  );
}
```

```typescript
// apps/web/src/components/chat/chat-main.tsx

'use client';

import { useChatStore } from '@/lib/store/chat-store';
import { MessageList } from './message-list';
import { MessageInput } from './message-input';
import { ModelSelector } from './model-selector';

export function ChatMain({ className }: { className?: string }) {
  const { currentSession, isStreaming, streamingContent } = useChatStore();

  return (
    <div className={`flex flex-col ${className}`}>
      {/* ëª¨ë¸ ì„ íƒ í—¤ë” */}
      <div className="h-14 border-b px-4 flex items-center">
        <ModelSelector />
      </div>

      {/* ë©”ì‹œì§€ ëª©ë¡ */}
      <div className="flex-1 overflow-y-auto p-4">
        <MessageList
          messages={currentSession?.messages ?? []}
          streamingContent={isStreaming ? streamingContent : undefined}
        />
      </div>

      {/* ì…ë ¥ ì˜ì—­ */}
      <div className="border-t p-4">
        <MessageInput disabled={isStreaming} />
      </div>
    </div>
  );
}
```

```typescript
// apps/web/src/components/chat/message-list.tsx

'use client';

import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import rehypeHighlight from 'rehype-highlight';
import { ChatMessage } from '@/lib/store/chat-store';

interface MessageListProps {
  messages: ChatMessage[];
  streamingContent?: string;
}

export function MessageList({ messages, streamingContent }: MessageListProps) {
  return (
    <div className="space-y-4">
      {messages.map((message) => (
        <MessageBubble key={message.id} message={message} />
      ))}

      {streamingContent && (
        <MessageBubble
          message={{
            id: 'streaming',
            role: 'assistant',
            content: streamingContent,
            timestamp: Date.now(),
          }}
          isStreaming
        />
      )}
    </div>
  );
}

function MessageBubble({
  message,
  isStreaming,
}: {
  message: ChatMessage;
  isStreaming?: boolean;
}) {
  const isUser = message.role === 'user';

  return (
    <div className={`flex ${isUser ? 'justify-end' : 'justify-start'}`}>
      <div
        className={`max-w-[80%] rounded-lg px-4 py-2 ${
          isUser
            ? 'bg-primary text-primary-foreground'
            : 'bg-muted'
        }`}
      >
        <ReactMarkdown
          remarkPlugins={[remarkGfm]}
          rehypePlugins={[rehypeHighlight]}
          className="prose prose-sm dark:prose-invert"
        >
          {message.content as string}
        </ReactMarkdown>

        {!isUser && message.latency && (
          <div className="text-xs text-muted-foreground mt-2">
            TTFT: {message.latency}ms
          </div>
        )}

        {isStreaming && (
          <span className="inline-block w-2 h-4 bg-current animate-pulse ml-1" />
        )}
      </div>
    </div>
  );
}
```

### 4-4. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `/chat` í˜ì´ì§€ ë Œë”ë§
- [ ] ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´ ë™ì‘
- [ ] ë©”ì‹œì§€ ì…ë ¥ ë° ì „ì†¡
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤ì‹œê°„ í‘œì‹œ
- [ ] ë§ˆí¬ë‹¤ìš´/ì½”ë“œ í•˜ì´ë¼ì´íŒ…
- [ ] ëŒ€í™” ê¸°ë¡ ìœ ì§€
- [ ] íŒŒë¼ë¯¸í„° ì¡°ì ˆ (temperature ë“±)
- [ ] ìƒˆ ëŒ€í™” ì‹œì‘

---

## 5ë‹¨ê³„: Pass-through í”„ë¡ì‹œ

### ëª©í‘œ
- CORS ë¯¸ì§€ì› ì œê³µìë¥¼ ìœ„í•œ í”„ë¡ì‹œ ì„œë²„ êµ¬í˜„
- API í‚¤ ë¹„ì €ì¥ ì›ì¹™ ì¤€ìˆ˜

### ì˜ˆìƒ ì†Œìš”: 3-4ì¼

### 5-1. FastAPI ê¸°ë³¸ êµ¬ì¡°

```python
# apps/proxy/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from routers import completions
from middleware.security import no_key_logging_middleware

app = FastAPI(
    title="openMaaS Proxy",
    description="Zero-Knowledge Pass-through Proxy for LLM APIs",
    version="0.1.0",
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API í‚¤ ë¡œê¹… ë°©ì§€ ë¯¸ë“¤ì›¨ì–´
app.middleware("http")(no_key_logging_middleware)

# ë¼ìš°í„° ë“±ë¡
app.include_router(completions.router, prefix="/v1")

@app.get("/health")
async def health_check():
    return {"status": "healthy", "key_storage": "none"}
```

### 5-2. ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´

```python
# apps/proxy/middleware/security.py

import logging
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

# API í‚¤ê°€ ë¡œê·¸ì— ë‚¨ì§€ ì•Šë„ë¡ í•„í„°ë§
class KeyFilter(logging.Filter):
    SENSITIVE_HEADERS = {'x-api-key', 'authorization', 'x-aws-access-key'}

    def filter(self, record: logging.LogRecord) -> bool:
        if hasattr(record, 'msg'):
            msg = str(record.msg).lower()
            for header in self.SENSITIVE_HEADERS:
                if header in msg:
                    record.msg = '[REDACTED - Contains sensitive header]'
                    break
        return True

# ëª¨ë“  ë¡œê±°ì— í•„í„° ì ìš©
for handler in logging.root.handlers:
    handler.addFilter(KeyFilter())

async def no_key_logging_middleware(request: Request, call_next):
    """
    API í‚¤ê°€ ë¡œê¹…ë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•˜ëŠ” ë¯¸ë“¤ì›¨ì–´.

    ì£¼ì˜: ì´ í”„ë¡ì‹œëŠ” API í‚¤ë¥¼ ì ˆëŒ€ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    í‚¤ëŠ” ìš”ì²­ í—¤ë”ì—ì„œ ì¶”ì¶œí•˜ì—¬ ì¦‰ì‹œ ì—…ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ë‹¬ë˜ê³ ,
    ì‘ë‹µ í›„ ë©”ëª¨ë¦¬ì—ì„œ íê¸°ë©ë‹ˆë‹¤.
    """
    # ìš”ì²­ ë¡œê¹… ì‹œ í—¤ë” ì œì™¸
    safe_headers = {
        k: v for k, v in request.headers.items()
        if k.lower() not in {'x-api-key', 'authorization'}
    }

    response = await call_next(request)
    return response
```

### 5-3. Anthropic ì–´ëŒ‘í„° (í”„ë¡ì‹œìš©)

```python
# apps/proxy/adapters/anthropic.py

from typing import AsyncIterator
import httpx
from .base import ProxyAdapter, ChatRequest, ChatResponse

class AnthropicAdapter(ProxyAdapter):
    """
    Anthropic Claude API ì–´ëŒ‘í„°.

    ì£¼ì˜: API í‚¤ëŠ” ë©”ëª¨ë¦¬ì—ì„œë§Œ ì‚¬ìš©ë˜ê³  ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """

    BASE_URL = "https://api.anthropic.com/v1"

    def build_headers(self, api_key: str) -> dict:
        return {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }

    def transform_request(self, request: ChatRequest) -> tuple[str, dict]:
        """OpenAI í˜•ì‹ â†’ Anthropic í˜•ì‹"""

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ì¶œ
        system_content = ""
        messages = []

        for msg in request.messages:
            if msg["role"] == "system":
                system_content += msg["content"] + "\n"
            else:
                # OpenAIì˜ assistant â†’ Anthropicì˜ assistant
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"],
                })

        payload = {
            "model": request.model,
            "messages": messages,
            "max_tokens": request.max_tokens or 4096,
        }

        if system_content:
            payload["system"] = system_content.strip()

        if request.temperature is not None:
            payload["temperature"] = request.temperature

        if request.top_p is not None:
            payload["top_p"] = request.top_p

        if request.stream:
            payload["stream"] = True

        return f"{self.BASE_URL}/messages", payload

    def transform_response(self, response: dict) -> ChatResponse:
        """Anthropic í˜•ì‹ â†’ OpenAI í˜•ì‹"""

        content = ""
        for block in response.get("content", []):
            if block["type"] == "text":
                content += block["text"]

        return {
            "id": response["id"],
            "object": "chat.completion",
            "created": int(__import__("time").time()),
            "model": response["model"],
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                },
                "finish_reason": self._map_stop_reason(response.get("stop_reason")),
            }],
            "usage": {
                "prompt_tokens": response["usage"]["input_tokens"],
                "completion_tokens": response["usage"]["output_tokens"],
                "total_tokens": (
                    response["usage"]["input_tokens"] +
                    response["usage"]["output_tokens"]
                ),
            },
        }

    async def stream_transform(
        self,
        response: httpx.Response
    ) -> AsyncIterator[str]:
        """Anthropic SSE â†’ OpenAI SSE í˜•ì‹"""

        async for line in response.aiter_lines():
            if not line.startswith("data: "):
                continue

            data = line[6:]  # "data: " ì œê±°
            if data == "[DONE]":
                yield "data: [DONE]\n\n"
                break

            import json
            event = json.loads(data)

            # content_block_delta ì´ë²¤íŠ¸ë§Œ ì²˜ë¦¬
            if event.get("type") == "content_block_delta":
                delta_text = event.get("delta", {}).get("text", "")
                if delta_text:
                    chunk = {
                        "id": "anthropic-stream",
                        "object": "chat.completion.chunk",
                        "created": int(__import__("time").time()),
                        "model": "claude",
                        "choices": [{
                            "index": 0,
                            "delta": {"content": delta_text},
                            "finish_reason": None,
                        }],
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"

            elif event.get("type") == "message_stop":
                chunk = {
                    "id": "anthropic-stream",
                    "object": "chat.completion.chunk",
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop",
                    }],
                }
                yield f"data: {json.dumps(chunk)}\n\n"
                yield "data: [DONE]\n\n"

    def _map_stop_reason(self, reason: str | None) -> str:
        mapping = {
            "end_turn": "stop",
            "max_tokens": "length",
            "stop_sequence": "stop",
        }
        return mapping.get(reason, "stop")
```

### 5-4. Completions ë¼ìš°í„°

```python
# apps/proxy/routers/completions.py

from fastapi import APIRouter, Request, HTTPException
from fastapi.responses import StreamingResponse
import httpx
from adapters.anthropic import AnthropicAdapter
# from adapters.bedrock import BedrockAdapter  # Phase 2
# from adapters.azure import AzureAdapter      # Phase 2

router = APIRouter()

# ì œê³µìë³„ ì–´ëŒ‘í„° ë“±ë¡
ADAPTERS = {
    "anthropic": AnthropicAdapter(),
    # "bedrock": BedrockAdapter(),
    # "azure": AzureAdapter(),
}

@router.post("/chat/completions")
async def proxy_chat_completions(request: Request):
    """
    OpenAI í˜¸í™˜ Chat Completions í”„ë¡ì‹œ.

    í—¤ë”:
        X-Provider: ì œê³µì ID (anthropic, bedrock, azure ë“±)
        X-API-Key: ì œê³µì API í‚¤ (ì €ì¥ë˜ì§€ ì•ŠìŒ)

    ì£¼ì˜: API í‚¤ëŠ” ì´ í•¨ìˆ˜ ì‹¤í–‰ í›„ ë©”ëª¨ë¦¬ì—ì„œ íê¸°ë©ë‹ˆë‹¤.
    """

    # í—¤ë”ì—ì„œ ì œê³µìì™€ í‚¤ ì¶”ì¶œ
    provider = request.headers.get("X-Provider")
    api_key = request.headers.get("X-API-Key")

    if not provider:
        raise HTTPException(400, "X-Provider í—¤ë”ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    if not api_key:
        raise HTTPException(400, "X-API-Key í—¤ë”ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    adapter = ADAPTERS.get(provider)
    if not adapter:
        raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")

    # ìš”ì²­ ë³¸ë¬¸ íŒŒì‹±
    body = await request.json()

    # ìš”ì²­ ë³€í™˜
    endpoint, payload = adapter.transform_request(body)
    headers = adapter.build_headers(api_key)

    # ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ í™•ì¸
    is_streaming = body.get("stream", False)

    async with httpx.AsyncClient(timeout=120.0) as client:
        if is_streaming:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            response = await client.post(
                endpoint,
                json=payload,
                headers=headers,
                timeout=None,
            )

            if response.status_code != 200:
                raise HTTPException(response.status_code, response.text)

            return StreamingResponse(
                adapter.stream_transform(response),
                media_type="text/event-stream",
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            response = await client.post(endpoint, json=payload, headers=headers)

            if response.status_code != 200:
                raise HTTPException(response.status_code, response.text)

            result = adapter.transform_response(response.json())
            return result

    # í•¨ìˆ˜ ì¢…ë£Œ ì‹œ api_keyëŠ” ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œë¨
```

### 5-5. í´ë¼ì´ì–¸íŠ¸ í”„ë¡ì‹œ ì–´ëŒ‘í„°

```typescript
// apps/web/src/lib/llm/adapters/proxy.ts

import { LLMAdapter, ChatRequest, ChatResponse, ChatChunk } from './base';
import { ProviderId } from '@openmaas/shared-types';

export class ProxyAdapter implements LLMAdapter {
  readonly providerId = 'proxy' as const;
  readonly supportsCORS = true;  // í”„ë¡ì‹œ ìì²´ëŠ” CORS ì§€ì›

  private proxyUrl: string;

  constructor(proxyUrl: string) {
    this.proxyUrl = proxyUrl;
  }

  async chat(
    request: ChatRequest,
    apiKey: string,
    provider?: ProviderId
  ): Promise<ChatResponse> {
    const response = await fetch(`${this.proxyUrl}/v1/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-Provider': provider ?? '',
        'X-API-Key': apiKey,
      },
      body: JSON.stringify({ ...request, stream: false }),
    });

    if (!response.ok) {
      throw new Error(`Proxy error: ${response.status}`);
    }

    return response.json();
  }

  async *chatStream(
    request: ChatRequest,
    apiKey: string,
    provider?: ProviderId
  ): AsyncIterable<ChatChunk> {
    const response = await fetch(`${this.proxyUrl}/v1/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-Provider': provider ?? '',
        'X-API-Key': apiKey,
      },
      body: JSON.stringify({ ...request, stream: true }),
    });

    if (!response.ok) {
      throw new Error(`Proxy error: ${response.status}`);
    }

    const reader = response.body!.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const text = decoder.decode(value);
      const lines = text.split('\n').filter((line) => line.startsWith('data: '));

      for (const line of lines) {
        const data = line.slice(6);  // "data: " ì œê±°
        if (data === '[DONE]') break;

        try {
          yield JSON.parse(data) as ChatChunk;
        } catch {
          // íŒŒì‹± ì‹¤íŒ¨ ë¬´ì‹œ
        }
      }
    }
  }
}
```

### 5-6. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í”„ë¡ì‹œ ì„œë²„ ì‹¤í–‰ (`uvicorn main:app`)
- [ ] `/health` ì—”ë“œí¬ì¸íŠ¸ ì‘ë‹µ í™•ì¸
- [ ] Anthropic í‚¤ë¡œ Claude í˜¸ì¶œ ì„±ê³µ
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë™ì‘
- [ ] API í‚¤ê°€ ë¡œê·¸ì— ë‚¨ì§€ ì•ŠìŒ í™•ì¸
- [ ] í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í”„ë¡ì‹œ ê²½ìœ  í˜¸ì¶œ ì„±ê³µ

---

## 6ë‹¨ê³„: ë¹„ìš© ì¶”ì  + ë¹„êµ ëª¨ë“œ

### ëª©í‘œ
- í† í°/ë¹„ìš© ê³„ì‚° ë° í‘œì‹œ
- Side-by-Side ëª¨ë¸ ë¹„êµ

### ì˜ˆìƒ ì†Œìš”: 3-4ì¼

### 6-1. í† í° ì¹´ìš´í„°

```typescript
// apps/web/src/lib/llm/token-counter.ts

import { encoding_for_model, TiktokenModel } from 'tiktoken';
import { ProviderId } from '@openmaas/shared-types';

// ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ë§¤í•‘
const MODEL_TOKENIZERS: Record<string, TiktokenModel> = {
  'gpt-4o': 'gpt-4o',
  'gpt-4-turbo': 'gpt-4-turbo',
  'gpt-4': 'gpt-4',
  'gpt-3.5-turbo': 'gpt-3.5-turbo',
  // Claude, GeminiëŠ” ê·¼ì‚¬ì¹˜ ì‚¬ìš© (GPT-4 ê¸°ì¤€)
  'claude-3-5-sonnet': 'gpt-4',
  'claude-3-opus': 'gpt-4',
  'gemini-1.5-pro': 'gpt-4',
  'gemini-2.0-flash': 'gpt-4',
};

export function countTokens(text: string, model: string): number {
  const tokenizerModel = MODEL_TOKENIZERS[model] || 'gpt-4';

  try {
    const enc = encoding_for_model(tokenizerModel);
    const tokens = enc.encode(text);
    enc.free();
    return tokens.length;
  } catch {
    // í´ë°±: ëŒ€ëµì ì¸ ì¶”ì • (4ê¸€ìë‹¹ 1í† í°)
    return Math.ceil(text.length / 4);
  }
}

export function countMessageTokens(
  messages: { role: string; content: string }[],
  model: string
): number {
  let total = 0;

  for (const msg of messages) {
    // ë©”ì‹œì§€ ì˜¤ë²„í—¤ë“œ (role, formatting)
    total += 4;
    total += countTokens(msg.content, model);
  }

  // í”„ë¦¬ì•°ë¸” í† í°
  total += 3;

  return total;
}
```

### 6-2. ê°€ê²© ë°ì´í„°

```json
// packages/pricing/models.json

{
  "openai": {
    "gpt-4o": {
      "input": 2.5,
      "output": 10,
      "cached_input": 1.25
    },
    "gpt-4o-mini": {
      "input": 0.15,
      "output": 0.6
    },
    "gpt-4-turbo": {
      "input": 10,
      "output": 30
    }
  },
  "anthropic": {
    "claude-3-5-sonnet-20241022": {
      "input": 3,
      "output": 15
    },
    "claude-3-opus-20240229": {
      "input": 15,
      "output": 75
    }
  },
  "gemini": {
    "gemini-1.5-pro": {
      "input": 1.25,
      "output": 5
    },
    "gemini-2.0-flash-exp": {
      "input": 0,
      "output": 0
    }
  }
}
```

### 6-3. ë¹„ìš© ê³„ì‚° ìœ í‹¸ë¦¬í‹°

```typescript
// apps/web/src/lib/llm/pricing.ts

import pricingData from '@openmaas/pricing/models.json';
import { ProviderId } from '@openmaas/shared-types';

interface PricingInfo {
  input: number;   // $ per 1M tokens
  output: number;
  cached_input?: number;
}

export function getModelPricing(
  provider: ProviderId,
  model: string
): PricingInfo | null {
  const providerPricing = pricingData[provider as keyof typeof pricingData];
  if (!providerPricing) return null;

  return providerPricing[model as keyof typeof providerPricing] ?? null;
}

export function calculateCost(
  provider: ProviderId,
  model: string,
  inputTokens: number,
  outputTokens: number,
  cachedInputTokens: number = 0
): number {
  const pricing = getModelPricing(provider, model);
  if (!pricing) return 0;

  const inputCost = (inputTokens / 1_000_000) * pricing.input;
  const outputCost = (outputTokens / 1_000_000) * pricing.output;
  const cachedCost = pricing.cached_input
    ? (cachedInputTokens / 1_000_000) * pricing.cached_input
    : 0;

  return inputCost + outputCost + cachedCost;
}

export function formatCost(cost: number): string {
  if (cost < 0.01) {
    return `$${(cost * 100).toFixed(4)}Â¢`;
  }
  return `$${cost.toFixed(4)}`;
}
```

### 6-4. Side-by-Side ë¹„êµ UI

```typescript
// apps/web/src/app/compare/page.tsx

'use client';

import { useState } from 'react';
import { useChatStore } from '@/lib/store/chat-store';
import { providerRouter } from '@/lib/llm/provider-router';
import { calculateCost, formatCost } from '@/lib/llm/pricing';
import { countMessageTokens } from '@/lib/llm/token-counter';
import { ProviderId } from '@openmaas/shared-types';

interface ComparisonResult {
  provider: ProviderId;
  model: string;
  content: string;
  inputTokens: number;
  outputTokens: number;
  cost: number;
  ttft: number;        // Time to First Token (ms)
  totalTime: number;   // Total time (ms)
  error?: string;
}

export default function ComparePage() {
  const [prompt, setPrompt] = useState('');
  const [models, setModels] = useState<{ provider: ProviderId; model: string }[]>([
    { provider: 'openai', model: 'gpt-4o' },
    { provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' },
  ]);
  const [results, setResults] = useState<ComparisonResult[]>([]);
  const [isRunning, setIsRunning] = useState(false);

  const runComparison = async () => {
    if (!prompt.trim()) return;

    setIsRunning(true);
    setResults([]);

    const messages = [{ role: 'user' as const, content: prompt }];

    // ëª¨ë“  ëª¨ë¸ì— ë³‘ë ¬ë¡œ ìš”ì²­
    const promises = models.map(async ({ provider, model }) => {
      const result: ComparisonResult = {
        provider,
        model,
        content: '',
        inputTokens: countMessageTokens(messages, model),
        outputTokens: 0,
        cost: 0,
        ttft: 0,
        totalTime: 0,
      };

      const startTime = Date.now();
      let ttft = 0;

      try {
        for await (const chunk of providerRouter.chatStream(provider, {
          model,
          messages,
          temperature: 0.7,
          max_tokens: 1024,
          stream: true,
        })) {
          if (!ttft) {
            ttft = Date.now() - startTime;
          }

          const delta = chunk.choices[0]?.delta?.content ?? '';
          result.content += delta;

          // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
          setResults((prev) => {
            const idx = prev.findIndex(
              (r) => r.provider === provider && r.model === model
            );
            if (idx >= 0) {
              const updated = [...prev];
              updated[idx] = { ...result, ttft, content: result.content };
              return updated;
            }
            return [...prev, { ...result, ttft, content: result.content }];
          });
        }

        result.ttft = ttft;
        result.totalTime = Date.now() - startTime;
        result.outputTokens = countTokens(result.content, model);
        result.cost = calculateCost(
          provider,
          model,
          result.inputTokens,
          result.outputTokens
        );
      } catch (error) {
        result.error = error instanceof Error ? error.message : 'Unknown error';
      }

      return result;
    });

    await Promise.all(promises);
    setIsRunning(false);
  };

  return (
    <div className="container py-8">
      <h1 className="text-3xl font-bold mb-8">ëª¨ë¸ ë¹„êµ</h1>

      {/* í”„ë¡¬í”„íŠ¸ ì…ë ¥ */}
      <div className="mb-6">
        <textarea
          value={prompt}
          onChange={(e) => setPrompt(e.target.value)}
          placeholder="ë¹„êµí•  í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
          className="w-full h-32 p-4 border rounded-lg"
        />
        <button
          onClick={runComparison}
          disabled={isRunning || !prompt.trim()}
          className="mt-2 px-6 py-2 bg-primary text-primary-foreground rounded-lg disabled:opacity-50"
        >
          {isRunning ? 'ë¹„êµ ì¤‘...' : 'ë¹„êµ ì‹¤í–‰'}
        </button>
      </div>

      {/* ê²°ê³¼ ë¹„êµ */}
      <div className="grid grid-cols-2 gap-4">
        {results.map((result) => (
          <div key={`${result.provider}-${result.model}`} className="border rounded-lg p-4">
            <div className="font-semibold mb-2">
              {result.provider} / {result.model}
            </div>

            {result.error ? (
              <div className="text-red-500">{result.error}</div>
            ) : (
              <>
                <div className="prose prose-sm max-h-64 overflow-y-auto mb-4">
                  {result.content}
                </div>

                <div className="text-sm text-muted-foreground space-y-1">
                  <div>ì…ë ¥: {result.inputTokens} í† í°</div>
                  <div>ì¶œë ¥: {result.outputTokens} í† í°</div>
                  <div>ë¹„ìš©: {formatCost(result.cost)}</div>
                  <div>TTFT: {result.ttft}ms</div>
                  <div>ì´ ì‹œê°„: {result.totalTime}ms</div>
                </div>
              </>
            )}
          </div>
        ))}
      </div>

      {/* ë¹„êµ ìš”ì•½ í…Œì´ë¸” */}
      {results.length > 1 && !isRunning && (
        <div className="mt-8">
          <h2 className="text-xl font-semibold mb-4">ë¹„êµ ìš”ì•½</h2>
          <table className="w-full border-collapse border">
            <thead>
              <tr className="bg-muted">
                <th className="border p-2">ëª¨ë¸</th>
                <th className="border p-2">ë¹„ìš©</th>
                <th className="border p-2">TTFT</th>
                <th className="border p-2">ì´ ì‹œê°„</th>
                <th className="border p-2">ì¶œë ¥ í† í°</th>
              </tr>
            </thead>
            <tbody>
              {results
                .filter((r) => !r.error)
                .sort((a, b) => a.cost - b.cost)
                .map((result, idx) => (
                  <tr key={`${result.provider}-${result.model}`}>
                    <td className="border p-2">
                      {idx === 0 && 'ğŸ† '}
                      {result.model}
                    </td>
                    <td className="border p-2">{formatCost(result.cost)}</td>
                    <td className="border p-2">{result.ttft}ms</td>
                    <td className="border p-2">{result.totalTime}ms</td>
                    <td className="border p-2">{result.outputTokens}</td>
                  </tr>
                ))}
            </tbody>
          </table>
        </div>
      )}
    </div>
  );
}
```

### 6-5. ë¹„ìš© í‘œì‹œ ì»´í¬ë„ŒíŠ¸ (ì±„íŒ… UIìš©)

```typescript
// apps/web/src/components/chat/cost-display.tsx

import { useChatStore } from '@/lib/store/chat-store';
import { calculateCost, formatCost } from '@/lib/llm/pricing';
import { countMessageTokens } from '@/lib/llm/token-counter';

export function CostDisplay() {
  const { currentSession, provider, model } = useChatStore();

  if (!currentSession?.messages.length) {
    return null;
  }

  let totalInputTokens = 0;
  let totalOutputTokens = 0;

  for (const msg of currentSession.messages) {
    const tokens = countTokens(msg.content as string, model);
    if (msg.role === 'user' || msg.role === 'system') {
      totalInputTokens += tokens;
    } else {
      totalOutputTokens += tokens;
    }
  }

  const cost = calculateCost(provider, model, totalInputTokens, totalOutputTokens);

  return (
    <div className="flex items-center gap-4 text-sm text-muted-foreground">
      <span>ì…ë ¥: {totalInputTokens} í† í°</span>
      <span>ì¶œë ¥: {totalOutputTokens} í† í°</span>
      <span className="font-medium">ë¹„ìš©: {formatCost(cost)}</span>
    </div>
  );
}
```

### 6-6. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í† í° ì¹´ìš´í„° ì •í™•ë„ í™•ì¸
- [ ] ë¹„ìš© ê³„ì‚° ì •í™•ë„ í™•ì¸
- [ ] ì±„íŒ… UIì— ë¹„ìš© í‘œì‹œ
- [ ] `/compare` í˜ì´ì§€ ë Œë”ë§
- [ ] 2ê°œ ëª¨ë¸ ë™ì‹œ ë¹„êµ ì‹¤í–‰
- [ ] ë¹„êµ ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
- [ ] TTFT ì¸¡ì • ì •í™•ë„

---

## ì „ì²´ MVP ì™„ë£Œ ì¡°ê±´

- [ ] OpenAI, Gemini, Ollama ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥
- [ ] Anthropic í”„ë¡ì‹œ ê²½ìœ  í˜¸ì¶œ ê°€ëŠ¥
- [ ] API í‚¤ ì…ë ¥/ì €ì¥/ì‚­ì œ
- [ ] ì±„íŒ… í”Œë ˆì´ê·¸ë¼ìš´ë“œ ê¸°ëŠ¥ ì™„ì„±
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í‘œì‹œ
- [ ] ë¹„ìš©/í† í° ì¶”ì 
- [ ] ëª¨ë¸ ë¹„êµ ê¸°ëŠ¥
- [ ] Docker Compose ë°°í¬ ê°€ëŠ¥

---

## ë‹¤ìŒ ë‹¨ê³„ (Phase 2 ë¯¸ë¦¬ë³´ê¸°)

- AWS Bedrock, Azure OpenAI, Vertex AI í”„ë¡ì‹œ ì–´ëŒ‘í„°
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì €ì¥/ê´€ë¦¬
- ëŒ€í™” ê¸°ë¡ ì˜êµ¬ ì €ì¥ (IndexedDB)
- ì‚¬ìš©ì ê³„ì • ì‹œìŠ¤í…œ
- íŒ€ ê³µìœ  ê¸°ëŠ¥
